# 强化学习GSPO算法

群组序列策略优化：GSPO如何解决大模型强化学习中的稳定性危机

## 引言

**强化学习训练中的"崩溃噩梦"**

在大语言模型的后训练阶段，强化学习技术一直是提升模型性能的关键环节。然而，工程师们常常面临一个噩梦般的场景：投入海量的计算资源，训练了数天甚至数周后，模型的性能突然"断崖式下跌"，一切努力付诸东流。这种现象被称为"模型崩溃"，已成为大模型训练中的一大痛点。

Qwen团队最近在7月份提出了一种名为GSPO（Group Sequence Policy Optimization，群组序列策略优化）的创新算法，旨在彻底解决这一问题。 该算法已成功应用于最新的Qwen3模型训练中，标志着大模型强化学习技术的一次重大突破。

## 传统强化学习

传统方法的致命缺陷：GRPO为何频频崩溃？

在GSPO出现之前，GRPO（Group Relative Policy Optimization）是广泛应用于大模型强化学习的主流算法。然而，随着模型规模的扩大，GRPO暴露出了严重的稳定性问题。

GRPO的核心问题在于其"Token级优化"的设计基因。在千亿参数、万Token序列的LLM训练中，这种逐token的优化方式会导致训练曲线从平稳上升瞬间暴跌，模型仿佛被"格式化"——这就是GRPO训练的崩溃常态。

更深层次的问题是，GRPO在长期训练中会暴露出严重的不稳定性问题并招致不可逆转的模型崩溃，这直接阻碍了通过增加计算资源来获得进一步性能提升的可能性。 这种崩溃不是偶然现象，而是GRPO算法设计中的根本性缺陷所导致的。

## GSPO：从序列级视角重构强化学习

Qwen团队通过深入分析发现，问题的根源在于传统方法过度关注单个token的优化，而忽视了整个序列的连贯性和一致性。GSPO的突破性创新正是从这一点出发，将优化视角从"token级"提升到"序列级"。

### 核心思想

GSPO的核心思想是：**不再逐词反馈，而是关注整体表现**。当模型不再被要求在每个token上都达到最优，而是着眼于整个输出序列的质量时，训练过程变得更加稳定，模型也能学得更强。

具体来说，GSPO采用了群组序列策略优化的方法，将样本分组处理，每组内进行序列级别的策略优化，避免了传统方法中因单个token错误导致整个训练过程崩溃的风险。

### 技术优势

GSPO的贡献远超增量性改进。通过识别并修复GRPO中的根本性设计缺陷，该算法为稳定且可扩展的强化学习建立了更加坚实的理论基础。 具体技术优势包括：

1. **序列级优化目标**：GSPO实现了数学目标与序列级优化的精确匹配，避免了token级优化带来的累积误差。
2. **稳定性增强**：通过群组处理机制，GSPO显著降低了训练过程中的方差，使训练曲线更加平滑稳定。
3. **无需路由回放**：与某些需要复杂机制的方法不同，GSPO不需要MoE模型的路由回放，并允许进行标记级别的微调。
4. **计算效率提升**：GSPO在保持高性能的同时，减少了不必要的计算开销，使训练过程更加高效。

## GSPO与GRPO：关键区别解析

| 特性         | GRPO                     | GSPO                   |
| ------------ | ------------------------ | ---------------------- |
| **优化粒度** | Token级                  | 序列级                 |
| **稳定性**   | 容易崩溃                 | 高度稳定               |
| **理论基础** | 存在设计缺陷             | 建立了更坚实的理论基础 |
| **训练曲线** | 波动剧烈，可能断崖式下跌 | 平稳上升，持续优化     |
| **适用场景** | 小规模模型               | 千亿参数级大模型       |

关键区别在于，GRPO试图在每个token位置都达到最优，而GSPO则关注整个输出序列的全局质量。这种视角的转变解决了大模型训练中的根本性问题。

## 实战效果：Qwen3背后的秘密武器

GSPO已成功应用于Qwen3的训练中，成为其性能提升的关键因素之一。 在实际应用中，GSPO展现出以下显著效果：

- **训练稳定性大幅提升**：Qwen3的训练过程不再出现"断崖式下跌"，模型性能持续稳定提升。
- **性能边界拓展**：通过稳定的训练过程，Qwen3能够充分利用更多计算资源，实现性能的进一步突破。
- **泛化能力增强**：由于关注整体序列质量，Qwen3在各种复杂任务上的表现更加连贯和可靠。

特别值得一提的是，GSPO的稳定性优势使得Qwen团队能够进行更长时间的训练，充分挖掘模型潜力，这是之前使用GRPO时无法实现的。

## 技术细节：GSPO如何工作？

虽然完整的算法细节需要参考Qwen团队的原始论文，但从现有资料可以推断GSPO的核心工作流程：

1. **序列分组**：将输入样本按相似性或难度分组，形成训练批次。
2. **序列级奖励计算**：为整个输出序列计算奖励，而非每个token单独计算。
3. **群组策略优化**：在组内进行策略更新，确保更新方向一致且稳定。
4. **自适应KL控制**：动态调整KL散度约束，平衡新旧策略之间的差异，防止过大更新导致崩溃。

这种设计确保了模型在学习过程中不会因为单个错误决策而陷入崩溃，而是能够从整体上逐步优化其行为策略。

## 未来展望：GSPO的潜在影响

GSPO的出现不仅解决了Qwen3训练中的实际问题，更为整个大模型强化学习领域指明了新的方向：

1. **序列级优化将成为主流**：未来的大模型强化学习可能会更多关注整体输出质量，而非局部最优。
2. **理论基础的完善**：GSPO的成功将推动更多关于序列级强化学习理论的研究。
3. **跨领域应用**：这种稳定性增强的方法可能被应用于其他需要序列生成的AI领域，如代码生成、音乐创作等。

## 结语

GSPO代表了大模型强化学习技术的一次重要进步。通过将优化视角从token级提升到序列级，Qwen团队成功解决了困扰行业已久的"模型崩溃"问题，为大模型的持续优化铺平了道路。

对于AI研究者和工程师而言，理解并掌握GSPO的原理不仅有助于提升模型训练的稳定性，更能深化对大模型行为模式的理解。随着更多团队采用这一技术，我们有望看到更加稳定、强大且可靠的大语言模型不断涌现。

正如Qwen团队所展示的：当AI的"教学方法"发生根本性变革时，模型的学习效果也会随之发生质的飞跃。GSPO正是这一理念的完美体现——关注整体表现，而非纠结于每个细节，让模型在稳定中不断进步。